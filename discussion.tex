%!TEX root=paper.tex

\section{Discussion}

\todo{use this subsection to discuss limitations (from the previous evaluation section) and add the need to be able to handle the horizontal scaling of the application; remove sectioning and summarize briefly limitations that have already been discussed in the VISSOFT paper/move material to other sections; consider renaming as limitations or something similar}


- The main goal of the \tool design was to allow analytics to be collected and insight to be gained by making the smallest possible changes to a running API. %To allow the collection of evolutionary information 

- We can do this minimal configuration for any technology. We showed here how to do it for Python and Flask because they are some of the most popular web development technologies at the moment. however, nothing prevents generalization. sure, for different technologies, several components have to be adapted: 
-- endpoint discovery -- this will be done differently for different technologies
-- deploying the dashboard as a new endpoint -- this will be also done differently for different technologies 
--   A further possible extension point here is supporting other version control systems (e.g. Mercurial). However, this is a straightforward extension.


- could even start with this: NOTHING PREVENTS GENERALIZATION
In this article we take as an example the Flask case study
and of the \tool as a dashboard. 
But there is nothing that prevents the generalization of 
our approach to other languages and technologies.


- Lanza has done nice stuff for source code evolution patterns. we need more work on automatic detection of service / endpoint evolution patterns. as exemplified by the utilization evolution.



- A Special Type of Outlier: Exceptions. TODO: probably something for the journal extension ...
  statistics about which endpoints fail most often might be useful too.
  same information as the outlier maybe?

- time as measured in days and commits are two sides of the same coin. 

- currently the system supports only one grouping. this need not be so, and there are many scenarios where one could envision multiple groupins in parallel.

- threats to the validity of the constant-load performance testing. performance regressions? 
- the endpoint where the unit-testing results can be uploaded is prone to being tampered with by Bob who would want to upload junk information to mess up with the dashboard. to solve this, a UUID can also be added that is generated by the server, and thus prevents anybody else but the rightful owner of the dashboard 

- user management. although not explained in the paper, the dashboard also comes with a user management system. and two classes of users. admins and guests.

- Although predicting the performance of the application would require advanced statistical work, it can be succesfully used as an advanced warning system for the overall performance trend for given endpoints.

- detecting minor versions which don't need to be tracked, since they didn't touch the performance of the system. E.g. a modification of the README, etc. 

- discuss later one situation in which opt-out might be desirable. 

- tracking other types of information: exceptions, etc. especially since logs usually get lost... nobody has time to look in them...

-  In this paper we present the integration with APIs written in Python and Flask hoping that this will not prevent the reader from seeing the more general idea; all the tools we show here for Flask can be applied to other API technologies (e.g. Django) by simply providing a few back-end adapters in the right places.

- We need more advanced outlier analysis tools... Instead of manually searching in the page to build stats about Google vs. Microsoft

- branches. the system does not care. it links to whatever commit is the current one. it could be that visualizations could be developed to comapre branches, but this optiona. future work. 

- outlier detection and monitoring is important. manual exploration is critical. however, one needs a performant way of analyzing multiple logs. in the example we saw we had to use the search facility to investigate our hypothesis that Google Translate was slower than Microsoft Translate. Moreover, these trends might change too... 

- Performance. We have provided a mechanism for measuring performance, and showed a glimpse into the overhead imposed by the first version of the Dashboard. For our case study the overhead is acceptable, but for performance critical applications, it might be too large. One of the downsides of the current implementation is the fact that the dashboard runs in the same process as the main application and that Python only supports green threads. The performance could be greatly improved if instead the Dashboard would be run in a different process, and the information would be posted to it by using a messaging service. However, that would complicate the installation and configuration, and for this kind of architecture, there are also other alternatives.

- We designed the dashboard to make it easy to adopt. However, we still have to run a study on the challenges and opportunities of the adoption of such a dashboard. 


-   Supporting situations where the API is deployed across multiple containers for example.

-   DETECTING ENDPOINT RENAMES.  The history would normally be lost in the case of an API rename. (also known as the {\em provenance issue} in source code evolution analysis)
    In theory we could infer that an API endpoint was renamed if it is not to be found anymore, and another one with very similar timing characteristics would be found. Such a detection can never be completely sure, but it would help with meeting the needs of the API maintainer... 







  \subsection*{Limitations of the Current Evolution Approach}
  This technique assumes that the web application code which is the target of the monitoring is deployed using \git in the following way: 

  \begin{enumerate}
    \item The deployment engineer pulls the latest version of the code from the integration server; this will result in a new commit being pointed at by the HEAD pointer. %than previously
    \item The deployment engineer restarts the new version of the service. At this point, the \tool detects that a new HEAD is present in the local code base and consequently starts associating all the new data points with this new commit\footnote{The \tool detects the current version of the analyzed system the first time it is attached to the application object, and thus, assumes that the Flask application is restarted when a new version is deployed. This is in tune with the current version of Flask, but if the web server will support dynamic updates in the future, this might have to be taken into account}.
  \end{enumerate}

    The advantage of this approach is the need for minimal configuration effort, as discussed in the presentation of the tool. The disadvantage is that it will consider on equal ground the smallest of commits, even one that modifies a comment, and the shortest lived of commits, e.g.~a commit which was active only for a half an hour before a new version with a bug fix was deployed, with major and minor releases of the software. %as a distinct way of grouping the data points. 
  A mechanism to control which versions are important for monitoring purposes is therefore required to be added.
%


  \subsection*{Limitation of Visualizations }

    The visualizations for the user experiene perspectives as presented in Section \ref{sec:user} have been tested with several hundred users (of which about \activeUserCount were active during the course of the study), but the scalability of the visualizations must be further investigated for web services with tens of thousands of users.


  \subsection*{Allowing Multiple Groupings}

  One of the current limitations is that one can only specify one type of grouping. 
  However, there could be cases where multiple groupins would be desirable. 
  
  Grouping the information by client application that sends the request could be important as a complementary grouping even in the current \zee case study. 

