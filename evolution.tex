%!TEX root=paper.tex
  
  \section{Version-Aware Monitoring}
  
  The Stack Overflow Developer Survey from 2018 revealed the fact that 88.4\% of professional developers use git for version control. 
  Given that versioning using git is the main way in which the source code of services evolves, it is natural to monitor their progress also across versions. 

  Version control in \tool can be supported in two ways,
  the developer explicitly states the current version, 
  or the current version is automatically detected based
  on some version control system. 


  % However, with the current configuration of the tool, it would be impossible for the maintainer to see the improvements resulting from the optimization. 

  If assume that the code that they run is deployed using .git, then with an extra line of configuration they can allow \tool to find the git\footnote{\url{https://git-scm.com/}} folder of the deployed service and automatically detect the version of the project that is running: 
    
\begin{lstlisting}[style=custompython]

# LOC #3: provide the dashboard with 
# information on where to find the 
# git information 
dashboard.config.git = 'path/to/.git'
  
      
\end{lstlisting}  
 
  If the \tool can automatically detect the current version of the project by reading the .git configuration as soon as the API is started and can then group measurements by version\footnote{Alternatively, the maintainer can add version identifiers manually for the web application through a configuration file if the system does not use git.}. 


  

  \subsection*{Evolving Utilization}

  \Fref{fig:mv-util} shows the \perspective{Multi-Version API Utilization} perspective which presents the utilization of the tracked endpoints across versions. Because the different versions might be deployed for very different periods of time, at the intersection of an endpoint and a version the chart does not plot the absolute utilization of that endpoint in that version but rather the percentage of all the API calls that go to that endpoint. Otherwise a version that is deployed for many weeks would make all those deployed for several days invisible.


    \begin{figure}[h!]
      \centering
      \includegraphics[width=0.9\linewidth]{utilization-evolution}
      \caption{The Evolution of All the API Endpoints Utilization Across System Versions}
      \label{fig:mv-util}
    \end{figure}

  With this view, several patterns are visible:
  \begin{itemize}
    
    \item \epFeedItems is discontinued a little bit after \epTopArticles is being introduced

  \end{itemize}


  \subsection*{Evolving Performance}

    \Fref{fig:tee} is a zoomed-in version of such a view for \epTranslations with versions increasing from top to bottom

    \begin{figure}[h!]
      \centering
      \includegraphics[width=0.9\linewidth]{translation_endpoint_evolution_}
      \caption{The Performance Evolution of the \epTranslations endpoint}
      \label{fig:tee}
    \end{figure}


  This view confirms that the performance of the translation endpoint improved in the recent versions: the median of the last three versions is constantly moving towards the left, and progresses from 1.4 seconds (in the top-most box plot in \Fref{fig:tee}) to 0.8 in the latest version (bottom-most box plot).


\subsection*{Evolving Groups}
  The limitation of the previous view is that it does not present the information also on a per version basis. To address this, a different visual perspective entitled \perspective{Multi-Version per-User Performance} can be defined. Figure \ref{fig:tuv} presents such a perspective by mapping the average execution time for a given user (lines) and given version (columns) on the area of the corresponding circle. 

\begin{figure}[h!]
  \centering
  \includegraphics[width=0.97\linewidth]{time_per_user_per_version}
  \caption{This perspective shows that the evolution of response times for individual users (horizontal lines) across versions (the x-axis) for a given endpoint}
  \label{fig:tuv}
\end{figure}


The colors represent users. The figure shows average performance varying  across users and versions with no clear trend: this is probably because varying user workload (i.e. number of sources to which the user is registered) is the reason for the variation in response times. \ins{one can see that for user 1 performance degrades over the versions. Given that for other users the performance does not degrade in the same way, it is probable that the problem might lay elsewhere: the server was overloaded or the endpoint is ``algorithmically slower''.}


  
