%!TEX root=paper.tex

\section{Discussion}

-   \ins{We can do this minimal configuration for any technology. We showed here how to do it for Python and Flask because they are some of the most popular web development technologies at the moment.}


- A Special Type of Outlier: Exceptions. TODO: probably something for the journal extension ...
  statistics about which endpoints fail most often might be useful too.
  same information as the outlier maybe?

- time as measured in days and commits are two sides of the same coin. 

- currently the system supports only one grouping. this need not be so, and there are many scenarios where one could envision multiple groupins in parallel.

- threats to the validity of the constant-load performance testing. performance regressions? 
- the endpoint where the unit-testing results can be uploaded is prone to being tampered with by Bob who would want to upload junk information to mess up with the dashboard. to solve this, a UUID can also be added that is generated by the server, and thus prevents anybody else but the rightful owner of the dashboard 

- user management. although not explained in the paper, the dashboard also comes with a user management system. and two classes of users. admins and guests.

- Although predicting the performance of the application would require advanced statistical work, it can be succesfully used as an advanced warning system for the overall performance trend for given endpoints.

- detecting minor versions which don't need to be tracked, since they didn't touch the performance of the system. E.g. a modification of the README, etc. 

- discuss later one situation in which opt-out might be desirable. 




\todo{use this subsection to discuss limitations (from the previous evaluation section) and add the need to be able to handle the horizontal scaling of the application; remove sectioning and summarize briefly limitations that have already been discussed in the VISSOFT paper/move material to other sections; consider renaming as limitations or something similar}

  \subsection{Distributed System Monitoring}
  \todo{To think about}
  Supporting situations where the API is deployed across multiple containers for example.


  \subsubsection{Automatically Monitoring System Evolution}

  The main goal of the \tool design was to allow analytics to be collected and insight to be gained by making the smallest possible changes to a running API. %To allow the collection of evolutionary information 
%
  This technique assumes that the web application code which is the target of the monitoring is deployed using \git in the following way: 

  \begin{enumerate}
    \item The deployment engineer pulls the latest version of the code from the integration server; this will result in a new commit being pointed at by the HEAD pointer. %than previously
    \item The deployment engineer restarts the new version of the service. At this point, the \tool detects that a new HEAD is present in the local code base and consequently starts associating all the new data points with this new commit\footnote{The \tool detects the current version of the analyzed system the first time it is attached to the application object, and thus, assumes that the Flask application is restarted when a new version is deployed. This is in tune with the current version of Flask, but if the web server will support dynamic updates in the future, this might have to be taken into account}.
  \end{enumerate}

  The advantage of this approach is the need for minimal configuration effort, as discussed in the presentation of the tool. The disadvantage is that it will consider on equal ground the smallest of commits, even one that modifies a comment, and the shortest lived of commits, e.g.~a commit which was active only for a half an hour before a new version with a bug fix was deployed, with major and minor releases of the software. %as a distinct way of grouping the data points. 
  A mechanism to control which versions are important for monitoring purposes is therefore required to be added.
%
  A further possible extension point here is supporting other version control systems (e.g. Mercurial). However, this is a straightforward extension.

  \subsubsection{NEW: API Renames}
  \ml{just an idea}


    The history would normally be lost in the case of an API rename. (also known as the {\em provenance issue} in source code evolution analysis)
    In theory we could infer that an API was renamed if it is not to be found anymore, and another one with very similar timing characteristics would be found. Such a detection can never be completely sure, but it would help with meeting the needs of the API maintainer... 

  \subsubsection{User-Awareness }

    For the situations in which the user information is not available, the \tool tracks by default information about different IPs and in some cases this might be a sufficiently good approximation of the user diversity and identity. 
    %

    The visualizations for the user experiene perspectives as presented in Section \ref{sec:user} have been tested with several hundred users (of which about \activeUserCount were active during the course of the study), but the scalability of the visualizations must be further investigated for web services with tens of thousands of users.


  \subsubsection{Other Possible Groupings}

    There are other groupings of service utilization and performance that could be important to the maintainer, that we did not explore in this paper. For example, if the service is using OAuth, then together with every request, in the header of the request there is information about the application which is sending a request. Grouping the information by application that sends the request could be important in such a context. 

    In general, providing a mechanism that would allow very easy specification of groupings (either as code annotations, as normal code, or as configuration options) is an open problem that \tool and any other similar library will have to face.

