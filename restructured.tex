\documentclass[conference]{IEEEtran}
	\pdfpagewidth=8.5truein
	\pdfpageheight=11truein

\usepackage{graphicx}
\usepackage{subfig}
\usepackage[bookmarks=false]{hyperref}
\usepackage{xspace}
\usepackage{listings}
\usepackage[usenames, dvipsnames]{color}
\usepackage{amssymb}


\usepackage{booktabs}         

\graphicspath{{./img/}}

%\hyphenation{op-tical net-works semi-conduc-tor}

\input{useful_defines}


\begin{document}
	

%
\title{Agile Monitoring of Evolving Web Services}

\author{
\IEEEauthorblockN{The Author, The Other Author}\\
Institute\\
University, Country\\
Email: \{author\}@university.domain 
}


% make the title area
\maketitle

\input{introductory}

\input{loc1}

\input{utilization}

\input{performance}

\input{grouping}

\input{evolution}

\input{testing}

\input{architecture}




  

\section{Performance Perspectives}
\label{sec:views}


\ins{PERFORMANCE DISCUSSION: this introspection, which looks up endpoints, happens only once at the startup of the service, so it is going to affect the API startup performance, but not the actual endpoint performance. In our case, the overhead here was quite small: TO MEASURE}.



  %In the remainder of the paper we present several of these perspectives.

  % The second endpoint conists of two parts, one of them being a table that shows for every monitored endpoint the number of hits it has gotten, the time it was last accessed and its average execution time. The second part is a view with four graphs which show:
  % \begin{itemize} 
  %   \item A heatmap of the total number of requests to the monitored endpoints
  %   \item A stacked bar chart that shows the total number of requests to the monitored endpoints per endpoint per day
  %   \item A boxplot graph showing the average execution time per version of the web service
  %   \item A boxplot graph showing the average execution time for every monitored endpoint
  % \end{itemize}

\todo{evaluate which figures to keep and ideally take new screenshots for the ones we keep; consider adding two more views (like Fig. 3.2 of Patrick's thesis)}


  



% In the Zeeguu case study, one of the slowest endpoints, and one with the highest variability as shown in \Fref{fig:ep} is \epFeedItems: it retrieves a list of recommended articles for a given user. However, since a user can be subscribed to anything from one to three dozen article sources, and since the computation of the difficulty is personalized and it is slow, the variability in time among users is likely to be very large. 





% \niceseparator

%\section{Preemptive Monitoring}
%\label{sec:testing}
%
%%\todo{discuss here idea of using unit testing as an early performance indicator for evolving services (`preemptive monitoring'?); add a side-by-side figure comparing unit testing vs live system performance per version, maybe calculate error in trend curves? find a better term than prediction for the section title (plus introduction)}
%
%\todo{Similar to the idea of augmenting service monitoring with online testing~\cite{metzger2010proactive}, i.e.~testing service-based applications by using dedicated test input in parallel to its normal use and operation. The difference is that we take advantage of the capability of Travis to create an emulated live environment for integration testing purposes, and use unit testing as the dedicated test input in order to measure performance. What we are going provide evidence for in the following is that we can indeed use this preemptive monitoring concept as an early performance indicator for evolving services.}
%
%The developer needs to define the unit test folder of the project, the URL that Travis should use for reporting its results, and the number of iterations for each unit test should be executed in order to generate 


\section{Evaluation}
\label{sec:evaluation}

%\todo{evaluation of the \tool across the following dimensions: ease of use, effectiveness, overhead to the system; for the last part we need to do the overhead measurement experiment, for the other two we base it on the case study and bring over whatever is necessary from the discussion section}



\input{discussion}

\input{related-work}


\section{Conclusion and Future Work}
\label{sec:conclusions}

\todo{update as appropriate}

In this paper we have shown that it is possible to create a monitoring solution which provides basic insight into web service utilization and performance  with very little effort from the developer. The user group that we are aiming for with this work is application developers using Flask and Python to build web applications with limited or no budget for implementing their own monitoring solutions. The emphasis is in allowing such users to gain insight into how the performance of the service evolves together with the application itself. We believe that the same architecture, and lessons can be applied to other frameworks and other languages.

In the future, we plan to perform case studies with other sytstems, with the goal of discover other needs and to wean out the less useful visualizations in the \tool. We plan to also extend the tool towards supporting multiple deployments of the same applications across multiple nodes (e.g. for the situations where the application is deployed together with a load balancer). Finally, we plan to integrate \tool with unit testing as a complementary source of information about performance evolution.



\newpage
\appendix

  \subsection{Changing the Default /dashboard Endpoint }

  THIS STUFF SHOULD BE MOVED TO THE APPENDIX. And in a footnote we just mention that ... \ins{REQUIREMENT}: The system must provide an easy way to add more specific configurations if needed. A way of overwriting the common sense defaults.

  Further configuration is not needed possible by adding additional statements before the binding definition, for example,

  \begin{lstlisting}[style=custompython]
  ...
  dashboard.config.link = 'mydashboard'
  dashboard.bind(flask_app)
  \end{lstlisting}
  
  allows for a custom route (\code{/mydashboard}) to the dashboard to be defined by the programmer. An external configuration file can be used instead, or in addition to this:
  
  \begin{lstlisting}[style=custompython]
  ...
  dashboard.config.from_file('config.cfg')
  ...
  \end{lstlisting}


  \subsection{For those who don't use GIT}


    \textit{Manual} version control requires the developer to tag each new version of the application with an appropriate version identifier~\cite{papazoglou2011managing} using the \code{APP\_VERSION} configuration parameter, for example by adding to the configuration file:
    
      \begin{lstlisting}[style=custompython]
      [dashboard]
      APP_VERSION=<versionID>
      \end{lstlisting}





% references section

\bibliographystyle{abbrv}
\bibliography{paper}


% that's all folks
\end{document}


