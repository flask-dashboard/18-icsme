\documentclass[conference]{IEEEtran}
	\pdfpagewidth=8.5truein
	\pdfpageheight=11truein

\usepackage{graphicx}
\usepackage{subfig}
\usepackage[bookmarks=false]{hyperref}
\usepackage{xspace}
\usepackage{listings}
\usepackage[usenames, dvipsnames]{color}

\usepackage{booktabs}         

\graphicspath{{./img/}}

%\hyphenation{op-tical net-works semi-conduc-tor}

\input{useful_defines}


\begin{document}
	

%
\title{Agile Monitoring of Evolving Web Services}

\author{
\IEEEauthorblockN{Patrick Vogel, Thijs Klooster, Vasilios Andrikopoulos, Mircea Lungu}\\
Johann Bernoulli Institute for Mathematics and Computer Science\\
University of Groningen, Netherland\\
Email: \{t.klooster.1,p.p.vogel\}@student.rug.nl, \{v.andrikopoulos,m.f.lungu\}@rug.nl 
}


% make the title area
\maketitle

\begin{abstract}
\todo{Update abstract \& title}
  Tens of thousands of web applications are written in Flask, a Python-based web framework. Despite a rich ecosystem of extensions, there is none that supports the developer in gaining insight into the evolving performance of their service. In this paper, we introduce \tool, a library that addresses this problem. We present the ease with which the library can be integrated in an already existing web application, discuss some of the visualization perspectives that the library provides and point to some future challenges for similar libraries.

\end{abstract}




\section{Introduction}

\todo{Intro to be rewritten; refer to VISSOFT paper as previous work, explain focus is on integration with version control/CI for evolution support, and use of unit tests as early performance indicator}

\todo{motivate with: monitoring in a world which is becoming more and more service-oriented is essential because it allows to perform three main types of actions: system adaptations to provide the request service at the desired level of quality, enabling flexibility in dealing with changing requirements and modes of operation, and enabling operational awareness through dashboards organizing the collected data~\cite{pernici2016monitoring}. In this work we focus on the last part and we discuss how a minimal effort probe-based solution for a particular type of web services can be used to facilitate the other two types of actions by involving the developer.}

\todo{explain: in~\cite{vogel2017low} we introduced the \tool an a high level with a focus on presenting its performance visualization aspects; in this work we only summarize these features by means of introducing some of the provided functionalities of the \tool. We focus on discussing how the tool is implemented and operating, provide a deeper look at its capabilities for integration with version control and continuous integration environments, introduce a mechanism for performance prediction, and provide an evaluation of the proposed approach based on a case study. }

%Every system is a distributed system nowadays \cite{cavage2013there}. Indeed a very large number of applications and web applications are nowadays implemented as two-tier architectures with a front-end implemented with web technologies and a service back-end.
%\ml{I'm not completely happy with this paragraph}
{\em There is no getting around it: you are building a distributed system} argues a recent article \cite{cavage2013there}. Indeed, even the simplest second-year student project is a web application implemented as two-tier architecture with a Javascript/HTML5 front-end a service backend, usually a REST API.

% \hfill mds
% Many contemporary programming languages are offering libraries, modules, or frameworks that facilitate the development of such architectures. 
Python is one of the most popular programming language choices for implementing the back-end of web applications. GitHub contains more than 500K open source Python projects and the Tiobe Index\footnote{TIOBE programming community index is a measure of popularity of programming languages, created and maintained by the TIOBE Company based in Eindhoven, the Netherlands} ranks Python as the 4th most popular programming language as of June 2016.
 
% possible flask summary
Within the Python community, Flask\footnote{\url{http://flask.pocoo.org/}} is a very popular web framework\footnote{More than 25K projects on GitHub (5\% of all Python projects) are implemented with Flask (cf. a GitHub search for ``language:Python Flask'')}. It provides simplicity and flexibility by implementing a bare-minimum web server, and thus advertises as a micro-framework. The Flask tutorial shows how setting up a simple Flask {\em ``Hello World''} web-service requires no more than 5 lines of Python code \cite{ flask:tutorial}.
% end of summary
 
Despite their popularity, to the best of our knowledge, there is no simple solution for monitoring the evolving performance of Flask web applications. Thus, every one of the developers of these projects faces one of the following options when confronted with the need of gathering insight into the runtime behavior of their implemented services: 

  \begin{enumerate}

    \item Use a commercial monitoring tool which treats the subject API as a black-box (e.g. Pingdom, Runscope). 
    % , Graphite+Graphana+statd etc.

    \item Implement their own ad-hoc analytics solution, having to reinvent basic visualization and interaction strategies. 

    \item Live without analytics insight into their services.

  \end{enumerate}

%\todo{For the first point in the list, we can also argue that analytics solutions like Google Analytics can be used, but they have no notion of versioning/integration with the development life cycle. Feel free to cite \cite{papazoglou2011managing} for service evolution purposes}

For projects on a budget (e.g. research, startups) the first and the second options are often not available due to time and financial constraints. Even when using 3rd-party analytics solutions, a critical insight into the evolution of the exposed services of the web application, is missing because such solutions have no notion of versioning and no integration with the development life cycle.~\cite{papazoglou2011managing}

To avoid projects ending up in the third situation, that of living without analytics, in this paper we present \tool~ --- a low-effort service monitoring library for Flask-based Python web services that is easy to integrate and enables the {\em agile assessment} of service evolution. \cite{Nier12b}

As a case study, on which we will illustrate our solution, we are going to use an open source API which, for several years, was in the third of the above-presented situations.

% In the next section, we will present a case study of an open source research API which was for a long time in the third situation presented above -- deployed without analytics insight.

\todo{Add signposting paragraph}



\section{Case Study: The API (Zeeguu)}
\label{sec:case}

%\todo{Either leave as is or add more info on usage by college if possible}

  \zee\footnote{\url{https://github.com/zeeguu-ecosystem/}} is a platform and an ecosystem of applications for accelerating vocabulary acquisition in a foreign language \cite{Lungu16}. 
%
  The architecture of the ecosystem has at its core an API and a series of satellite applications that together offer to a learner three main inter-dependent features:

  \begin{enumerate}

    \item Reader applications that provide effortless translations for those texts which are too difficult for the readers.

    \item Interactive exercises personally generated based on the preferences and past likes of the learner.

    \item Article recommendations which are personalized for the interests of the learner and come with difficulty estimation that helps the learner find articles with the appropriate difficulty.

  \end{enumerate}

  The core API implemented with Flask and Python provides correspondingly three types of functionality: contextual translations, article recommendations, and personalized exercise suggestions. In total the API provides a bit less than 50 endpoints, out of which probably a dozen are very frequently used. 
  % The development of the core API itself is a research project. 

  At the time of writing this article, the ecosystem consists of a reader web application, a web based exercises platform, and a smartwatch application, which are used at the moment of writing this article by more than \activeUserCount active users. The users come from a highschool, a language school, and some users are using it on their own, without any educational context. The highest load we observed until now on the API consisted of 12K requests in one day.

  
  We will use this \zee API as a case study for this paper. 
  All the figures in this paper are captured from the actual deployment of \tool in the context of the \zee API. The figures are interactive offering basic data exploration capabilities: filter, zoom, and details on demand\cite{Shne99a}. The \tool deployment for the case study can be accessed publicly\footnote{\url{https://zeeguu.unibe.ch/api/dashboard}; username: {\em guest}, password: {\em soap-sac}}. 

 % \todo{create new account, update URL if necessary, update activeUserCount, add a couple of sentences of the current state of the case study with the college/language center}
% \ml{we should consider adding also one section in which the architecture/implementation and main features of the dashboard are presented before going on with discussing them in more depth in the following sections --- this should include a rundown on which views are provided from where (overview or per endpoint)}

\newpage



\section{Case Study: The Technology (Flask)}
\label{sec:tool}

 Flask is a microframework for Python. It IS USED BY THOUSANDS OF PROJECTS. ELABORATE MORE ON THIS... 
 \vspace{2cm}

 Flask depends on two external libraries: the Jinja2 template engine and the Werkzeug WSGI toolkit. 

 “Micro” does not mean that your whole web application has to fit into a single Python file (although it certainly can), nor does it mean that Flask is lacking in functionality. The “micro” in microframework means Flask aims to keep the core simple but extensible. Flask won’t make many decisions for you, such as what database to use. Those decisions that it does make, such as what templating engine to use, are easy to change. Everything else is up to you, so that Flask can be everything you need and nothing you don’t.

 Flask has many configuration values, with sensible defaults, and a few conventions when getting started. By convention, templates and static files are stored in subdirectories within the application’s Python source tree, with the names templates and static respectively. While this can be changed, you usually don’t have to, especially when getting started.

 A Python web application based on WSGI has to have one central callable object that implements the actual application. In Flask this is an instance of the Flask class. Each Flask application has to create an instance of this class itself and pass it the name of the module, but why can’t Flask do that itself?

\begin{lstlisting}[style=custompython]

from flask import Flask
app = Flask(__name__)

@app.route('/')
def index():
    return 'Hello World!'

\end{lstlisting}


\subsection*{NOTHING PREVENTS GENERALIZATION}
In this article we take as an example the Flask case study.
But there is nothing that prevents the generalization of 
our approach. 

MAKE SURE THAT THIS IS CLEAR.




  \newpage
  \section{One Line Of Code Configuration With Technology Specific Interceptors}
  
  The fact that the \tool itself is being developed in Python using Flask makes binding to the services of a to-be monitored application developed with the same technologies easy and intuitive. 

  \ins{In general, what matters is that with the right backend, we can do this minimal configuration for any technology... }

  To start using the \tool, one simply needs to one lines of code\footnote{we don't count the import statement that enables that line...} to their Flask web service\footnote{\ins{ In this paper we present the integration with APIs written in Python and Flask hoping that this will not prevent the reader from seeing the more general idea; all the tools we show here for Flask can be applied to other API technologies (e.g. Django) by simply providing a few back-end adapters in the right places. }}:

  % caption=Configuring the \tool is straightforward,
  \begin{lstlisting}[style=custompython]
  import dashboard

  # flask_app is the Flask app object
  dashboard.bind(flask_app)
  \end{lstlisting}

  \ins{The dashboard can take advantage of the fact that it is monitoring an API... so it can simply just add one extra endpoint there.} After (re)deployment of the application, the interactive visualizations of the \tool become available at the \code{/dashboard} route of the Flask application. 


  During binding, the \tool will search for all endpoints defined in the target application. 

  In order to monitor an endpoint, the \tool creates a function wrapper for the API function that corresponds to the endpoint. This way, the wrapper will be executed whenever that API call is made before the actual function is called. The wrapper contains the code that takes care of monitoring an endpoint. Data collected by the wrappers are persisted in a local database. The SQLAlchemy Object Relational Mapper\footnote{\url{https://www.sqlalchemy.org/}} on top of SQLite\footnote{\url{https://www.sqlite.org/}} is used for this purpose.
  \ins{DISCUSSION about how do to this in other languages? Java: AspectJ probably, etc.}


  These will be presented to the user in the tool web interface, where the user can select the ones that should be monitored, see \Fref{fig:sep}. 

    \begin{figure}[h!]
      \centering
      \includegraphics[width=\linewidth]{selecting_endpoints.png}
      \caption{Once connected to an API the Dashboard presents the endpoints that are available for monitoring}
      \label{fig:sep}
    \end{figure}

  This is the alternative to using annotations like other white-box tools (e.g. the other flask performance plugin). 



  \ins{PROMISE:} The last access time of the endpoint by users, irrespective of whether it is selected by the user to be monitored or not, is also displayed as the means of identifying potentially obsolete endpoints.



  \ml{we must insert at least one dashboard view here... otherwise people will lose their patience... also it will help with explaining the outliers... also, one should see that a large variety of views are already available at this point... I know this risks being  }



In the Zeeguu case study, one of the slowest endpoints, and one with the highest variability as shown in \Fref{fig:ep} is \epFeedItems: it retrieves a list of recommended articles for a given user. However, since a user can be subscribed to anything from one to three dozen article sources, and since the computation of the difficulty is personalized and it is slow, the variability in time among users is likely to be very large. 



\begin{figure*}[h!]
  \centering
  \subfloat[number of requests per endpoint per day]{\includegraphics[width=.9\columnwidth]{number_of_requests_}\label{fig:aeu}}
  \subfloat[requests per hour heatmap]{\includegraphics[width=\columnwidth]{daily_patterns_}\label{fig:dp}}
  \caption{Some of the available views\label{fig:views}}
\end{figure*}

\newpage
\section{Service Utilization}
\label{sec:util}

\ins{A discussion must be added here about the types of useful information and problems that can be solved with these views.}

\textit{Service Utilization} presents information about the usage of all endpoints of interest to the developer,

The most fundamental insight that a service maintainer needs regards service utilization. %\vspace{0.5cm}

Simplest utilization perspective:

\begin{figure}[h!]
  \centering
  \includegraphics[width=0.5\linewidth]{endpoint-call-count}
  \caption{Caption here}
  \label{fig:figure1}
\end{figure}

% REQUIREMENT: Adding a time interval is important. 
% Otherwise the performance numbers in chart 1 are not good enough... also an outlier very far in the past, will make all the heatmap data points to look very pale in comparison forever after.


Figure \ref{fig:aeu} shows a first perspective on endpoint utilization that \tool provides: a stacked bar chart of the number of hits to various endpoints grouped by day\footnote{Endpoint colors are the same in different views}. Figure~\ref{fig:aeu} in particular shows that at its peak the API has about 12.000 hits per day. 

The way users interact with the platform can also be inferred since the endpoints are indicators of different activity types, e.g.: 

\begin{enumerate}
  
  \item {\color{myred} \epTranslations} is an indicator of the amount of foreign language reading the users are doing, and 
  
  \item {\color{mygreen} \epOutcome} is an indicator of the amount of foreign vocabulary practice the users are doing.
  
\end{enumerate}


%\begin{figure}[!ht]
% \centering
% \includegraphics[width=\linewidth]{number_of_requests_}
% \caption{The number of requests per endpoint per day view shows the overall utilization of the monitored application}
% \label{fig:aeu}
%\end{figure}

Besides showing the overall utilization, this endpoint provides the maintainer with information relevant for decisions regarding endpoint deprecation --- one of the most elementary ways of {\em understanding the needs of the downstream}\cite{Haen14a}. In our case study, the maintainer realized that one endpoint which they thought was not being used (i.e. \code{words\_to\_study}), contrary to their expectations, was actually being used\footnote{A complementary type of usage information can also be discovered in the view presented in Figure \ref{fig:sep} where seeing that an endpoint is never accessed can increase the confidence of the maintainer that a given endpoint is not used, although it can never be used a proof.}.





\niceseparator

%   \todo{Add the time series graph and discuss it before the heatmap? We can then sell the heatmap better} 
%   \ml{Not sure about which graph you refer to here V}

A second type of utilization question that the \tool can answer automatically regards {\em cyclic patterns of usage per hour of day} by means of a heatmap, as in \Fref{fig:dp}. 

% \mltp{ can we add vertical lines that highlight the beginning of a new week (e.g. before Sunday): }
% Patrick: adding separators in the graph is unfortunately not supported by the library.

%\begin{figure}[!ht]
% \centering
% \includegraphics[width=0.9\linewidth]{daily_patterns_}
% \caption{Usage patterns become easy to spot in the requests per hour heatmap}
% \label{fig:dp}
%\end{figure}


Figure \ref{fig:dp} shows the API not being used during the early morning hours, with most of the activity focused around working hours and some light activity during the evening. This is consistent with the fact that the current users are all in the central European timezone. Also, the figure shows that the spike in utilization that was visible also in the previous graph happended in on afternoon/evening.









\begin{figure*}[h!]
  \centering
  \subfloat[overview of the tracked endpoints]{\includegraphics[width=1.5\columnwidth]{overview}\label{fig:overview}}
  \caption{Some of the available views\label{fig:views}}
\end{figure*}


\newpage
\section{Endpoint Performance}
\label{sec:perf}


  The \tool also collects information regarding endpoint performance. The view in \Fref{fig:ep} summarizes the response times for various endpoints by using a box-and-whiskers plot. 


 \begin{figure}[!ht]
   \centering
   \includegraphics[width=0.95\linewidth]{endpoint_performance_}
   \caption{The response time (in ms) per monitored endpoint view allows for identifying performance variability and balancing issues}
   \label{fig:ep}
 \end{figure}

  From this view it became clear to the maintainer that four of the endpoints had very large variation in performance.   The most critical for the application and consequently the one optimized first was the \epTranslations endpoint which was part of an interactive loop in the reader applications that relied on the Zeeguu API. Moreover, cf. \Fref{fig:aeu} this endpoint is one of the most used in the system.



  \begin{figure*}[h!]
    \centering
    \includegraphics[width=\linewidth]{outliers}
    \caption{Automatically collected outlier information}
    \label{fig:figure1}
  \end{figure*}
  \newpage
  \subsection{Performance Insight: Automated outlier detection and monitoring}
  
  When an API is called from within a highly interactive application (as it is the case with the case study in this paper) 
  of particular interest to the API developers are performance {\em outliers}. 

  Indeed, a translation request that takes three times more than expected can seriously decrease the perceived quality of the application. Thus, identifying, collecting all appropriate data, and diagnosing the root causes of such outliers is especially critical in improving the quality of an application. 
  
  % In the context of the RESTful services support by Flask, this means request serving times that deviate from the average to an unexpected degree. 
  
  For this purpose the \tool tracks for every monitored endpoint a {\em running average} response time value\footnote{\ins{For performance reasons, we assume that the response times for the endpoints are normally distributed. Otherwise, more general density distribution information must be collected in real time.}}. When it detects that a given request is an outlier with respect to this past average running value, it triggers the {\em outlier data collection routine} which stores \ins{extra information} about the current execution environment. A configurable threshold with a default value of $2.5$ times the running average response time is used for this purpose. 

  For every detected outlier request, the \tool collects information about the current Python stack trace, CPU load, memory consumption, request parameters, etc. in order to allow the maintainer to investigate the causes of these exceptionally slow response times. In this way it is possible to get \ins{detailed insight into the operation of the application in the extreme cases without unnecessarily burdening it with logging this information for every request}.
  

The bottom panel shows the stack trace. 
In this particular case, it is revealing for the developer to learn that at the time of the stack trace snapshot, the code was in the microsoft\_translator.py: indeed, the system uses as back-end multiple translators, and it has been observed that many of the outliers happen to be waiting in the microsoft translator.

This information has to be corroborated with the observations that neither the memory nor the processor are overloaded at the moment. Thus this functionality in microsoft\_translator is really slow in itself, and this is not a result of the machine being overloaded for example. 



\subsection {A Special Type of Outlier: Exceptions}

   TODO: probably something for the journal extension ...
  statistics about which endpoints fail most often might be useful too.
  same information as the outlier maybe?







\begin{figure*}[h!]
  \centering
  \includegraphics[width=0.8\linewidth]{utilization-evolution}
  \caption{The Performance Evolution of the \epTranslations endpoint}
  \label{fig:tee}
\end{figure*}


  \newpage
  \section{Evolution monitoring }
  
  \begin{figure}[h!]
  \centering
  \includegraphics[width=0.8\linewidth]{translation_endpoint_evolution_}
  \caption{The Performance Evolution of the \epTranslations endpoint}
  \label{fig:tee}
\end{figure}

  When thinking about performance, 
  one can not avoid thinking about the performance evolution. 

  Version control in \tool can be supported in two ways,
  the developer explicitly states the current version, 
  or the current version is automatically detected based
  on some version control system. 


  However, with the current configuration of the tool, it would be impossible for the maintainer to see the improvements resulting from the optimization. 

  \ins{The latest STack Overflow Developer Survey: 88.4\% of **professional** developers use git.}  
  If assume that the code that they run is deployed using .git, then with an extra line of configuration they can allow \tool to find the git\footnote{\url{https://git-scm.com/}} folder of the deployed service and automatically detect the version of the project that is running: 
    
    \begin{lstlisting}[style=custompython]
    
      dashboard.config.git = 'path/to/.git'
      
    \end{lstlisting}  
 
  If the \tool can automatically detect the current version of the project by reading the .git configuration as soon as the API is started and can then group measurements by version\footnote{Alternatively, the maintainer can add version identifiers manually for the web application through a configuration file if the system does not use git.}. \Fref{fig:tee} is a zoomed-in version of such a view for \epTranslations with versions increasing from top to bottom
  

  This view confirms that the performance of the translation endpoint improved in the recent versions: the median of the last three versions is constantly moving towards the left, and progresses from 1.4 seconds (in the top-most box plot in \Fref{fig:tee}) to 0.8 in the latest version (bottom-most box plot).



  CHALLENGE: Supporting other types of version control... not real challenge, as we said 88percent of devs use git. 
  CHALLENGE 2: detecting minor versions which don't need to be tracked, since they didn't touch the performance of the system. E.g. a modification of the README, etc. 
  
\todo{TODO: endpoint evolution }



  

\begin{figure*}[!ht]
  \centering
  \includegraphics[width=0.4\linewidth]{time_per_user}
  \caption{The \epFeedItems shows a very high variability across users}
  \label{fig:tpu}
\end{figure*}


\newpage
\section{Grouping Requests}
\label{sec:user}



For service endpoints which run computations in real time, the maintainer of a system might want to understand the endpoint performance on a per-user basis, especially for situations where the system response time is a function of some individual user load\footnote{E.g. in GMail some users have two emails while other have twenty thousand and this induces different response times for different users}.

\todo{remove the configuration part and rephrase the following, the discussion has been partially subsumed by Section~\ref{sec:tool}.}

To enable this, the \tool must be configured to associate an API call with a given user. The simplest way is to take advantage of the architecture of Flask applications in which a global \code{flask.request} object can be used to retrieve the session which can in turn lead to user identification: 

%\begin{lstlisting}[float,caption=Simply define a custom app-specific function for user retrieval and pass it to the \tool to group information by user,style=custompython]
\begin{lstlisting}[style=custompython]  
# app specific way of extracting the user
# from a flask request object    
def get_user_id():
sid = int(flask.request.args['session'])
session = User.find_for_session(sid)
return user_id

# attaching the get_user_id function
dashboard.config.get_group_by = get_user_id

\end{lstlisting}

% \niceseparator


\todo{consider losing one or both of the figures --- or at least updating them}

In Zeeguu, \epFeedItems retrieves a list of recommended articles for a given user. Cf. \Fref{fig:ep} it is the endpoint with the slowest response time and highest variability. The reason for this is that a user can be subscribed to anything from one to three dozen article sources and for each of the sources the system must compute the personalized difficulty of each article at every request. 


A \perspective{Per-User Performance} perspective should show the different response times for different users. Figure \ref{fig:tpu} presents a subset of the corresponding view in the \tool. The figure shows that the response times for this endpoint can vary considerably for different users with some extreme cases where a user has to wait a full minute until their recommended articles are shown\footnote{After seeing this perspective, the maintainer refactored the architecture of the system to move part the difficulty computation out of the interactive loop}.



The limitation of the previous view is that it does not present the information also on a per version basis. To address this, a different visual perspective entitled \perspective{Multi-Version per-User Performance} can be defined. Figure \ref{fig:tuv} presents such a perspective by mapping the average execution time for a given user (lines) and given version (columns) on the area of the corresponding circle. 


The colors represent users. The figure shows average performance varying  across users and versions with no clear trend: this is probably because varying user workload (i.e. number of sources to which the user is registered) is the reason for the variation in response times. \ins{one can see that for user 1 performance degrades over the }

\begin{figure}[!ht]
  \centering
  \includegraphics[width=0.7\linewidth]{time_per_user_per_version}
  \caption{This perspective shows that the evolution of response times for individual users (horizontal lines) across versions (the x-axis)}
  \label{fig:tuv}
\end{figure}








  \newpage
  \section{Preemptive monitoring}

  \ins{INTRO - LINKING WITH PREVIOUS PARAGRAPH: the previous evolutionary view crucial for 
  understanding the impact of software evolution on performance.
  However, just by observing this one can't be sure whether 
  performance changes because of the code or because of the payload.}
  
  \todo{the following might need a bit of rewrite}
  
  The concept of {\em preemptive monitoring} of the application performance by means of instrumenting integration unit testing as the synthetic load is similar to the idea of augmenting service monitoring with online testing~\cite{metzger2010proactive}, i.e.~testing service-based applications by using dedicated test input in parallel to its normal use and operation. The difference is that we take advantage of the capability of the CI framework (i.e. Travis in this case) to create an emulated ``live'' environment for integration testing purposes, and use unit testing as the dedicated test input in order to measure performance. 
  
  For example, \Fref{fig:builds} is a screenshot from the dashboard showing the measured response times for 5 consequent builds, with 180 iterations of the unit tests executed in total across all endpoints. The outliers on the right of the figure are due to initial requests \ins{which must wait for a boot-up phase of the API}.  


    \begin{figure}[h!]
        \centering
        \includegraphics[width=\columnwidth]{travis_builds}
        \caption{Response times in 5 consequent Travis builds in the \zee case study}
        \label{fig:builds}
      \end{figure}

  
  While this integration testing environment is different from the actual production deployment one, and the load used is purely synthetic, as will show in Section~\ref{sec:evaluation}, it can actually be used successfully as an early performance indicator for the application developer. \ins{Although predicting the performance of the application would require advanced statistical work, it can be succesfully used as an advanced warning system for the overall performance trend for given endpoints}.  


  \paragraph{Integration with Travis}
  \ml{added new section... also, this has to be moved after introducing the pre-emptive monitoring since it's about running tests. switched the order}
  

  \ins{The pre-emptive monitoring system} can be configured to work together with Continuous Integration (CI) frameworks like Travis\footnote{\url{https://travis-ci.org/}} that deal with automated integration testing.
  The developer needs to define the unit test folder of the project, the URL that Travis should use for reporting its results, and the number of iterations for each unit test should be executed in order to generate a load for the application. 
  
  Each time a new build is created through Travis, the \tool automatically detects all available unit tests defined by the application developer and iterates through each one of them the number of times predefined by the developer while monitoring the response times for each request. The resulting measurements are persisted in a separate part of the tool database so as not to contaminate the ``live'' monitoring data. Beyond seeing the results of each Travis build in the \tool, this feature also allows for preemptive monitoring of the application, as we discuss in the following.    

  \ins{Besides functioning as an early warning system for performance, tracking the evolving performance of API tests serves a second purpose. To function as an anchor when the developer analyzes performance degradations. For example, when a developer sees that a given endpoint has become less performant after in a newly deployed version, they are able to investigate whether the performance degradation is visible also with the synthetic load. This would correspond to a performance degradation which is due to ``algorithmic performance degradation''. If on the other hand, the performance of the tests does not change between versions, the developer might conclude that the performance degradation could be due to the workload on the machines, or maybe to the workload mix of the users}
  
  %By these means, an application developer can use unit tests as synthetic loads for driving the performance monitoring of the application before it reaches the production phase.  As we will discuss further in Section~\ref{sec:testing}, this feature is integral in enabling preemptive performance monitoring of the application.  
  

\todo{here goes the discussion about our forecasting capabilities}

\begin{figure}[h!]
  \centering
  \subfloat[The reported response time (in ms) per deployed version in the observation period]{\includegraphics[width=.5\columnwidth]{response_per_version_trunced_trend}}
  \quad
  \subfloat[The measured response time (in ms) using integration with Travis]{\includegraphics[width=.46\columnwidth]{travis_builds_no_outliers_trend}}
  \caption{Comparison of the response times per endpoint: actual production system versus preemptive monitoring data}
  \label{fig:preemptive}
\end{figure}


\begin{table}[h]
  \begin{tabular}{lll}
    \toprule
    Iteration & \bfseries Live (median) & \bfseries Travis (median)\\
    \midrule
    1 & 1349.41 & 764.87\\ 
    2 & 1466.13 & 992.87\\
    3 & 1256.65 & 760.87\\
    4 & 1266.42 & 813.89\\
    5 & 1080.68 & 303.4\\
    \bottomrule
  \end{tabular}
\end{table}

Pearson correlation $r(3)=.93, p=.02$




    \begin{figure*}[ht!]
      \centering
      \includegraphics[width=0.8\linewidth]{interceptor}
      \caption{The first thing that needs to be done, is to decorate the application object with an INTERCEPTOR}
      \label{fig:sep}
    \end{figure*}

\newpage

\section{Monitoring Service Utilization}

REMEMBER THE GOAL: Our main goal with the \tool issues to allow the integration of the dashboard with an API with minimal effort.




\ins{The first important question that such a 
service monitoring system should provide is 
information about what endpoints are being
used and by whom}. 

TODO: Find references, arguments, related work
that provide more details about why understanding
who uses endpoints, and which are used are important.

This is clearly the case in static analysis (see paper
by haenni and lungu...) but should even more be the
case in APIs.




  \newpage
TODO: TALK ABOUT THE METAMODEL...


    \begin{figure}[ht!]
      \centering
        \includegraphics[width=0.8\linewidth]{db_schema}
        \caption{The model that supports the views presented in this paper}
        \label{fig:sep}
    \end{figure}

  As discussed in the introductory section, in previous work~\cite{vogel2017low} we introduced \tool, a drop-in Python library that allows developers to monitor their Flask-based Python web applications with minimal effort.
%
  The \tool is implemented for Python 3.6 and is available on the Python Package Index repository\footnote{\url{https://pypi.python.org/pypi/flask-monitoring-dashboard/1.8}} from where it can be installed by running \install from the command line. 
%  
  The source code of the \tool is published under a permissive MIT license and is available on GitHub\footnote{\url{https://github.com/flask-dashboard}}.
  
  




  

\section{Performance Perspectives}
\label{sec:views}


\ins{PERFORMANCE DISCUSSION: this introspection, which looks up endpoints, happens only once at the startup of the service, so it is going to affect the API startup performance, but not the actual endpoint performance. In our case, the overhead here was quite small: TO MEASURE}.



  %In the remainder of the paper we present several of these perspectives.\footnote{We recommend obtaining a color version of this paper for better readability}

  % The second endpoint conists of two parts, one of them being a table that shows for every monitored endpoint the number of hits it has gotten, the time it was last accessed and its average execution time. The second part is a view with four graphs which show:
  % \begin{itemize} 
  %   \item A heatmap of the total number of requests to the monitored endpoints
  %   \item A stacked bar chart that shows the total number of requests to the monitored endpoints per endpoint per day
  %   \item A boxplot graph showing the average execution time per version of the web service
  %   \item A boxplot graph showing the average execution time for every monitored endpoint
  % \end{itemize}

\todo{evaluate which figures to keep and ideally take new screenshots for the ones we keep; consider adding two more views (like Fig. 3.2 of Patrick's thesis)}


  



% In the Zeeguu case study, one of the slowest endpoints, and one with the highest variability as shown in \Fref{fig:ep} is \epFeedItems: it retrieves a list of recommended articles for a given user. However, since a user can be subscribed to anything from one to three dozen article sources, and since the computation of the difficulty is personalized and it is slow, the variability in time among users is likely to be very large. 





% \niceseparator

%\section{Preemptive Monitoring}
%\label{sec:testing}
%
%%\todo{discuss here idea of using unit testing as an early performance indicator for evolving services (`preemptive monitoring'?); add a side-by-side figure comparing unit testing vs live system performance per version, maybe calculate error in trend curves? find a better term than prediction for the section title (plus introduction)}
%
%\todo{Similar to the idea of augmenting service monitoring with online testing~\cite{metzger2010proactive}, i.e.~testing service-based applications by using dedicated test input in parallel to its normal use and operation. The difference is that we take advantage of the capability of Travis to create an emulated live environment for integration testing purposes, and use unit testing as the dedicated test input in order to measure performance. What we are going provide evidence for in the following is that we can indeed use this preemptive monitoring concept as an early performance indicator for evolving services.}
%
%The developer needs to define the unit test folder of the project, the URL that Travis should use for reporting its results, and the number of iterations for each unit test should be executed in order to generate 


\section{Evaluation}
\label{sec:evaluation}

%\todo{evaluation of the \tool across the following dimensions: ease of use, effectiveness, overhead to the system; for the last part we need to do the overhead measurement experiment, for the other two we base it on the case study and bring over whatever is necessary from the discussion section}



\section{Discussion}

\todo{use this subsection to discuss limitations (from the previous evaluation section) and add the need to be able to handle the horizontal scaling of the application; remove sectioning and summarize briefly limitations that have already been discussed in the VISSOFT paper/move material to other sections; consider renaming as limitations or something similar}

  \subsection{Distributed System Monitoring}
  \todo{To think about}
  Supporting situations where the API is deployed across multiple containers for example.


  \subsubsection{Automatically Monitoring System Evolution}

  The main goal of the \tool design was to allow analytics to be collected and insight to be gained by making the smallest possible changes to a running API. %To allow the collection of evolutionary information 
%
  This technique assumes that the web application code which is the target of the monitoring is deployed using \git in the following way: 

  \begin{enumerate}
    \item The deployment engineer pulls the latest version of the code from the integration server; this will result in a new commit being pointed at by the HEAD pointer. %than previously
    \item The deployment engineer restarts the new version of the service. At this point, the \tool detects that a new HEAD is present in the local code base and consequently starts associating all the new data points with this new commit\footnote{The \tool detects the current version of the analyzed system the first time it is attached to the application object, and thus, assumes that the Flask application is restarted when a new version is deployed. This is in tune with the current version of Flask, but if the web server will support dynamic updates in the future, this might have to be taken into account}.
  \end{enumerate}

  The advantage of this approach is the need for minimal configuration effort, as discussed in the presentation of the tool. The disadvantage is that it will consider on equal ground the smallest of commits, even one that modifies a comment, and the shortest lived of commits, e.g.~a commit which was active only for a half an hour before a new version with a bug fix was deployed, with major and minor releases of the software. %as a distinct way of grouping the data points. 
  A mechanism to control which versions are important for monitoring purposes is therefore required to be added.
%
  A further possible extension point here is supporting other version control systems (e.g. Mercurial). However, this is a straightforward extension.

  \subsubsection{NEW: API Renames}
  \ml{just an idea}


    The history would normally be lost in the case of an API rename. (also known as the {\em provenance issue} in source code evolution analysis)
    In theory we could infer that an API was renamed if it is not to be found anymore, and another one with very similar timing characteristics would be found. Such a detection can never be completely sure, but it would help with meeting the needs of the API maintainer... 

  \subsubsection{User-Awareness }

    For the situations in which the user information is not available, the \tool tracks by default information about different IPs and in some cases this might be a sufficiently good approximation of the user diversity and identity. 
    %

    The visualizations for the user experiene perspectives as presented in Section \ref{sec:user} have been tested with several hundred users (of which about \activeUserCount were active during the course of the study), but the scalability of the visualizations must be further investigated for web services with tens of thousands of users.


  \subsubsection{Other Possible Groupings}

    There are other groupings of service utilization and performance that could be important to the maintainer, that we did not explore in this paper. For example, if the service is using OAuth, then together with every request, in the header of the request there is information about the application which is sending a request. Grouping the information by application that sends the request could be important in such a context. 

    In general, providing a mechanism that would allow very easy specification of groupings (either as code annotations, as normal code, or as configuration options) is an open problem that \tool and any other similar library will have to face.



\section{Related Work}
\label{sec:related}

\todo{Refer the reader to~\cite{ghezzi2007run} and ~\cite{metzger2010analytical} for a more extensive discussion}

There is a long tradition of using visualization for gaining insight into software performance. Tools like Jinsight \cite{Pauw02a} and Web Services Navigator \cite{Pauw05} pioneered such an approach for Java and for Web Services that communicate with SOAP messages. Both have an ``omniscient'' view of the services / objects and their interactions. As opposed to them, in our work we present an analytics platform which focuses on monitoring a single Python web service from its own point of view.

From the perspective of service monitoring, our work falls within the server-side run-time monitoring of services ~\cite{ghezzi2007run}. While we don't implement the more advanced features of related monitoring solutions like QoS policies driving the monitoring, it presents nevertheless an easy to use approach support improving the performance of web applications. 

% \va{Mircea: Consider removing the rest for space...}
% An existing monitoring tool is Pingdom \footnote{https://www.pingdom.com/company/why-pingdom}, which monitors the uptime of an existing web-service. This tool works by pinging the websites (up to 60 times) every minute automatically. Thus this creates a lot of overhead and is bound to be noisy since it will also be influenced by the speed of the network connection\footnote{Another problem is that such a tool would }

% \todo{Runscope? Others?}


\section{Conclusion and Future Work}
\label{sec:conclusions}

\todo{update as appropriate}

In this paper we have shown that it is possible to create a monitoring solution which provides basic insight into web service utilization and performance  with very little effort from the developer. The user group that we are aiming for with this work is application developers using Flask and Python to build web applications with limited or no budget for implementing their own monitoring solutions. The emphasis is in allowing such users to gain insight into how the performance of the service evolves together with the application itself. We believe that the same architecture, and lessons can be applied to other frameworks and other languages.

In the future, we plan to perform case studies with other sytstems, with the goal of discover other needs and to wean out the less useful visualizations in the \tool. We plan to also extend the tool towards supporting multiple deployments of the same applications across multiple nodes (e.g. for the situations where the application is deployed together with a load balancer). Finally, we plan to integrate \tool with unit testing as a complementary source of information about performance evolution.



\newpage
\appendix

  \subsection{Changing the Default /dashboard Endpoint }

  THIS STUFF SHOULD BE MOVED TO THE APPENDIX. And in a footnote we just mention that ... \ins{REQUIREMENT}: The system must provide an easy way to add more specific configurations if needed. A way of overwriting the common sense defaults.

  Further configuration is not needed possible by adding additional statements before the binding definition, for example,

  \begin{lstlisting}[style=custompython]
  ...
  dashboard.config.link = 'mydashboard'
  dashboard.bind(flask_app)
  \end{lstlisting}
  
  allows for a custom route (\code{/mydashboard}) to the dashboard to be defined by the programmer. An external configuration file can be used instead, or in addition to this:
  
  \begin{lstlisting}[style=custompython]
  ...
  dashboard.config.from_file('config.cfg')
  ...
  \end{lstlisting}


  \subsection{For those who don't use GIT}


    \textit{Manual} version control requires the developer to tag each new version of the application with an appropriate version identifier~\cite{papazoglou2011managing} using the \code{APP\_VERSION} configuration parameter, for example by adding to the configuration file:
    
      \begin{lstlisting}[style=custompython]
      [dashboard]
      APP_VERSION=<versionID>
      \end{lstlisting}





% references section

\bibliographystyle{abbrv}
\bibliography{paper}


% that's all folks
\end{document}


