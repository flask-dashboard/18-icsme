%!TEX root=paper.tex

\section{Discussion}

  % \todo{use this subsection to discuss limitations (from the previous evaluation section) and add the need to be able to handle the horizontal scaling of the application; remove sectioning and summarize briefly limitations that have already been discussed in the VISSOFT paper/move material to other sections; consider renaming as limitations or something similar}


  The main goal of the \tool design was to allow analytics to be collected and insight to be gained by making the smallest possible changes to a running API. 

  In this paper we present the integration with APIs written in Python and Flask hoping that this will not prevent the reader from seeing the more general idea; all the tools we show here for Flask can be applied to other API technologies (e.g. Django) by simply providing a few back-end adapters in the right places.


  We can use the same approach for other technologies. We showed here how to do it for Python and Flask because they are some of the most popular web development technologies at the moment. However, we hope that this will not prevent the reader from seeing the more general idea. All the interactive perspectives we show here for Flask can be applied to other service technologies (e.g. Django) by simply providing a few back-end adapters in the right places (e.g. endpoint discovery, the \code{/dashboard} endpoint deployment). 

  However, the presented approach also comes with several limitations. Some of these limitations might also represent challenges for the future builders of similar tools.

  \subsection*{The Dashboard Overhead}

    We have provided a mechanism for measuring performance, and showed a glimpse into the overhead imposed by the first version of the Dashboard. For our case study the overhead is acceptable, but for performance critical applications, it might be too large. 

    One of the downsides of the current implementation is the fact that the dashboard runs in the same process as the main application and that Python only supports green threads. The performance could be greatly improved if instead the Dashboard would be run in a different process, and the information would be posted to it by using a messaging service. However, that would complicate the installation and configuration, and for this kind of architecture, there are also other alternatives.


  \subsection*{Limitations of the Measurements}
    One limitation of the benchmarking is that it does not use a more realistic mix of workload, but rather, measures individual endpoints. It might be that multiple concurrent and diverse endpoints would have different performance impact. In fact, based on the data in the dashboard one could actually construct a statistically relevant workload mix. 


  \subsection*{Lack of Empirical Proof of Ease of Adoptability}

    We designed the dashboard to make it easy to adopt and we think this is to a certain degree apparent. However, we have only presented a single use case where the API developers have used the dashboard. It would be worthwile to organize a dedicated study to observe the challenges and opportunities of the adoption of such a dashboard. It will also be important to see whether the perspectives presented here are relevant for other developers, and also which other perspectives are missing.


  % \subsection*{Limitation of Visualizations }

  %   The visualizations for the user experiene perspectives as presented in Section \ref{sec:user} have been tested with several hundred users (of which about \activeUserCount were active during the course of the study), but the scalability of the visualizations must be further investigated for web services with tens of thousands of users.


  \subsection*{Lack of Flexibility in Allowing Multiple Groupings}

    To be agile, a tool has to be easily adaptable to different usage scenarios. However, currently one of the current limitations of \tool is that that one can only specify one type of grouping. This is insufficient for the cases where multiple groupings are desirable. In the \zee case study this need emerged in the context in which it would be good to grouping the requests also by the client application that sends the request.

  \subsection*{Limitations of the Current Approach to Evolution Monitoring}

    This technique assumes that the web application code which is the target of the monitoring is deployed using \git in the following way: 

    \begin{enumerate}
      \item The deployment engineer pulls the latest version of the code from the integration server; this will result in a new commit being pointed at by the HEAD pointer. %than previously
      \item The deployment engineer restarts the new version of the service. At this point, the \tool detects that a new HEAD is present in the local code base and consequently starts associating all the new data points with this new commit\footnote{The \tool detects the current version of the analyzed system the first time it is attached to the application object, and thus, assumes that the Flask application is restarted when a new version is deployed. This is in tune with the current version of Flask, but if the web server will support dynamic updates in the future, this might have to be taken into account}.
    \end{enumerate}

      The advantage of this approach is the need for minimal configuration effort, as discussed in the presentation of the tool. The disadvantage is that it will consider on equal ground the smallest of commits, even one that modifies a comment, and the shortest lived of commits, e.g.~a commit which was active only for a half an hour before a new version with a bug fix was deployed, with major and minor releases of the software. %as a distinct way of grouping the data points. 
    A mechanism to control which versions are important for monitoring purposes is therefore required to be added.


  \subsection*{Monitoring Failures} 

    A Special Type of Outlier: Exceptions. TODO: probably something for the journal extension ...statistics about which endpoints fail most often might be useful too.
    same information as the outlier maybe?

    - tracking other types of information: exceptions, etc. especially since logs usually get lost... nobody has time to look in them...


  \subsection*{Multiple Containers}

  -   Supporting situations where the API is deployed across multiple containers for example.


  \subsection*{Performance Regressions}

    - Although predicting the performance of the application would require advanced statistical work, it can be succesfully used as an advanced warning system for the overall performance trend for given endpoints.

    - threats to the validity of the constant-load performance testing. performance regressions? 


    - time as measured in days and commits are two sides of the same coin. 


  \subsection*{Advanced Outlier Analysis}

    - We need more advanced outlier analysis tools... Instead of manually searching in the page to build stats about Google vs. Microsoft

    - outlier detection and monitoring is important. manual exploration is critical. however, one needs a performant way of analyzing multiple logs. in the example we saw we had to use the search facility to investigate our hypothesis that Google Translate was slower than Microsoft Translate. Moreover, these trends might change too... 


  \subsection*{Endpoint Provenance }

    -   DETECTING ENDPOINT RENAMES.  The history would normally be lost in the case of an API rename. (also known as the {\em provenance issue} in source code evolution analysis)
    In theory we could infer that an API endpoint was renamed if it is not to be found anymore, and another one with very similar timing characteristics would be found. Such a detection can never be completely sure, but it would help with meeting the needs of the API maintainer... 







% - the endpoint where the unit-testing results can be uploaded is prone to being tampered with by Bob who would want to upload junk information to mess up with the dashboard. to solve this, a UUID can also be added that is generated by the server, and thus prevents anybody else but the rightful owner of the dashboard 

% - user management. although not explained in the paper, the dashboard also comes with a user management system. and two classes of users. admins and guests.


% - detecting minor versions which don't need to be tracked, since they didn't touch the performance of the system. E.g. a modification of the README, etc. 

% - discuss later one situation in which opt-out might be desirable. 

% - branches. the system does not care. it links to whatever commit is the current one. it could be that visualizations could be developed to comapre branches, but this optiona. future work. 



  










%



